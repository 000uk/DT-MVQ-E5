import torch
import torch.nn as nn
import torch.nn.functional as F
from .backbones.e5_lora import E5LoRABackbone
from .heads.multi_vector import SimpleMultiVectorHead, MultiVectorHead

class BookEmbeddingModel(nn.Module):
    def __init__(self, model_name: str, lora_config: dict):
        super().__init__()
        self.backbone = E5LoRABackbone(model_name, lora_config)
        # self.head = MultiVectorHead(num_vectors=2, input_dim=self.backbone.config.hidden_size)
        self.head = SimpleMultiVectorHead(num_vectors=2, input_dim=self.backbone.config.hidden_size)
    
    def forward(self, input_ids, attention_mask, **kargs):
        sequence_output = self.backbone(input_ids, attention_mask) # (B, L, D)
        embeddings = self.head(sequence_output, attention_mask) # (B, k, D)
        return F.normalize(embeddings, p=2, dim=2) # contrastive loss ê³„ì‚°í•˜ë ¤ë©´ í•„ìˆ˜

class AdvancedModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = AutoModel.from_pretrained('intfloat/e5-small')
        
        # ğŸ”¥ [ê°„ì§€ í¬ì¸íŠ¸] ì¶”ê°€ì ì¸ Transformer Block (1~2ì¸µ)
        # E5ì˜ ì¶œë ¥ì„ í•œ ë²ˆ ë” ì •ì œí•´ì„œ "ë‚´ ë°ì´í„°ì…‹ ë§ì¶¤í˜•"ìœ¼ë¡œ ë§Œë“¦
        encoder_layer = nn.TransformerEncoderLayer(d_model=384, nhead=8, batch_first=True)
        self.context_block = nn.TransformerEncoder(encoder_layer, num_layers=2) # ë”± 2ì¸µë§Œ!

        # ìš°ë¦¬ê°€ ë§Œë“  ë©‹ì§„ Head
        self.head = CompetitiveVectorHead(num_vectors=2)

    def forward(self, input_ids, attention_mask):
        # 1. E5 (Giant)
        outputs = self.backbone(input_ids, attention_mask)
        sequence_output = outputs.last_hidden_state # (B, L, 384)

        # 2. Context Block (Adapter)
        # ì—¬ê¸°ì„œ í† í°ë¼ë¦¬ í•œ ë²ˆ ë” ì„ì´ë©´ì„œ "ì±… ì¶”ì²œ íŠ¹í™”" ë¬¸ë§¥ì„ ë§Œë“¦
        sequence_output = self.context_block(sequence_output, src_key_padding_mask=~attention_mask.bool())

        # 3. Head (Specialist)
        vectors = self.head(sequence_output)
        return vectors