import torch
import torch.nn as nn
import torch.nn.functional as F

class CompetitiveVectorHead(nn.Module):
    def __init__(self, num_vectors=2, input_dim=384, num_heads=8):
        super().__init__()
        self.num_vectors = num_vectors
        self.input_dim = input_dim
        self.num_heads = num_heads
        self.head_dim = input_dim // num_heads
        self.scale = self.head_dim ** -0.5

        # 1. í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ (Orthogonal Init í•„ìˆ˜!)
        self.query_tokens = nn.Parameter(torch.randn(1, num_vectors, input_dim))
        nn.init.orthogonal_(self.query_tokens)

        # 2. Linear Layers (Q, K, V)
        self.to_q = nn.Linear(input_dim, input_dim, bias=False)
        self.to_k = nn.Linear(input_dim, input_dim, bias=False)
        self.to_v = nn.Linear(input_dim, input_dim, bias=False)

        # 3. Output Projection & Norm
        self.to_out = nn.Linear(input_dim, input_dim)
        self.norm1 = nn.LayerNorm(input_dim)
        self.norm2 = nn.LayerNorm(input_dim)
        
        # 4. FFN (ê¸°ì¡´ê³¼ ë™ì¼)
        self.ffn = nn.Sequential(
            nn.Linear(input_dim, input_dim * 4),
            nn.GELU(),
            nn.Linear(input_dim * 4, input_dim)
        )

    def forward(self, seq_out, attn_mask=None):
        """
        seq_out: (B, L, D) - E5 Output
        """
        B, L, D = seq_out.shape
        K = self.num_vectors

        # 1. Q, K, V ìƒì„± ë° Head ë¶„ë¦¬
        # (B, K, H, Dh) í˜•íƒœë¡œ ë³€í™˜
        q = self.to_q(self.query_tokens.repeat(B, 1, 1)).reshape(B, K, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        k = self.to_k(seq_out).reshape(B, L, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        v = self.to_v(seq_out).reshape(B, L, self.num_heads, self.head_dim).permute(0, 2, 1, 3)

        # 2. Attention Score ê³„ì‚°
        # (B, H, K, Dh) @ (B, H, Dh, L) -> (B, H, K, L)
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale

        # ---------------------------------------------------------
        # ğŸ’ [í•µì‹¬] ì—¬ê¸°ê°€ ë°”ë¡œ Cherry-Pick í¬ì¸íŠ¸! ğŸ’
        # ---------------------------------------------------------
        # ì¼ë°˜ Attention: softmax(dim=-1) -> ë‹¨ì–´(L) ì¶•ìœ¼ë¡œ í™•ë¥  ê³„ì‚° (ëª¨ë‘ê°€ ê°™ì€ ë‹¨ì–´ ë´ë„ ë¨)
        # Competitive:    softmax(dim=-2) -> ì¿¼ë¦¬(K) ì¶•ìœ¼ë¡œ í™•ë¥  ê³„ì‚° (ë‹¨ì–´ í•˜ë‚˜ë¥¼ ë‘ê³  Kê°œê°€ ì‹¸ì›€)
        
        # í•´ì„: "ì…ë ¥ ë‹¨ì–´ í•˜ë‚˜(Key)ê°€ 1.0ì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ, v0ì™€ v1ì´ ë‚˜ëˆ  ê°€ì ¸ë¼!"
        # v0ê°€ 0.9 ê°€ì ¸ê°€ë©´ v1ì€ 0.1ë°–ì— ëª» ê°€ì ¸ê° -> ê°•ì œ ë¶„ë¦¬ íš¨ê³¼
        attn = dots.softmax(dim=-2) 
        
        # (ì„ íƒ) ì•ˆì •ì„±ì„ ìœ„í•œ ì •ê·œí™” (Slot Attention ë…¼ë¬¸ ë””í…Œì¼)
        # ê° ì¿¼ë¦¬ê°€ ê°€ì ¸ê°„ ì •ë³´ ì´ëŸ‰ìœ¼ë¡œ ë‚˜ëˆ ì¤Œ (ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šê²Œ)
        attn = attn / (attn.sum(dim=-1, keepdim=True) + 1e-8)

        # 3. Weighted Sum
        # (B, H, K, L) @ (B, H, L, Dh) -> (B, H, K, Dh)
        out = torch.matmul(attn, v)

        # 4. Reshape & Projection
        out = out.permute(0, 2, 1, 3).reshape(B, K, D)
        out = self.to_out(out)

        # 5. Residual & FFN
        # ê¸°ì¡´ ì¿¼ë¦¬ì— ë”í•´ì¤Œ (Perceiver ë°©ì‹)
        queries = self.query_tokens.repeat(B, 1, 1)
        x = self.norm1(queries + out)
        vectors = self.norm2(x + self.ffn(x))

        return vectors