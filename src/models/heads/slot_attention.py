class SlotAttentionHead(nn.Module):
    def __init__(self, num_vectors=2, input_dim=384, iters=3, hidden_dim=384):
        super().__init__()
        self.num_vectors = num_vectors
        self.iters = iters # ë³´í†µ 3ë²ˆ ì •ë„ ë°˜ë³µí•´ì„œ ê²½ìŸì‹œí‚´
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        self.scale = hidden_dim ** -0.5

        # 1. í•™ìŠµ ê°€ëŠ¥í•œ ì´ˆê¸° ìŠ¬ë¡¯ (ë‹˜ ì½”ë“œì˜ query_tokensì™€ ë™ì¼)
        # ì¤‘ìš”: mu(í‰ê· )ì™€ sigma(ë¶„ì‚°)ë¥¼ í•™ìŠµí•´ì„œ ë§¤ë²ˆ ìƒ˜í”Œë§í•˜ëŠ” ê²Œ ì›ë³¸ì´ì§€ë§Œ,
        # í…ìŠ¤íŠ¸ì—ì„œëŠ” ê·¸ëƒ¥ Parameterë¡œ ê³ ì •í•´ë„ ìž˜ ë©ë‹ˆë‹¤.
        self.slots_mu = nn.Parameter(torch.randn(1, num_vectors, input_dim))
        self.slots_log_sigma = nn.Parameter(torch.randn(1, num_vectors, input_dim))
        
        # 2. Linear Projections
        self.to_q = nn.Linear(input_dim, hidden_dim, bias=False)
        self.to_k = nn.Linear(input_dim, hidden_dim, bias=False)
        self.to_v = nn.Linear(input_dim, hidden_dim, bias=False)

        # 3. GRU (ë°˜ë³µ ì—…ë°ì´íŠ¸ì˜ í•µì‹¬)
        # ìŠ¬ë¡¯ì´ ì •ë³´ë¥¼ ë¨¹ê³  -> ì—…ë°ì´íŠ¸í•˜ê³  -> ë‹¤ì‹œ ì •ë³´ë¥¼ ë¨¹ëŠ” ê³¼ì •
        self.gru = nn.GRUCell(hidden_dim, input_dim)

        self.norm_input = nn.LayerNorm(input_dim)
        self.norm_slots = nn.LayerNorm(input_dim)
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, input_dim * 4),
            nn.ReLU(),
            nn.Linear(input_dim * 4, input_dim)
        )

    def forward(self, inputs, attn_mask=None):
        """
        inputs: (B, L, D) - E5ì˜ ì¶œë ¥
        """
        b, n, d = inputs.shape
        inputs = self.norm_input(inputs)
        
        # Key, ValueëŠ” ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘ 
        k = self.to_k(inputs) # (B, L, D)
        v = self.to_v(inputs) # (B, L, D)

        # ì´ˆê¸° ìŠ¬ë¡¯ ìƒì„± (Gaussian Sampling)
        mu = self.slots_mu.expand(b, self.num_vectors, -1)
        sigma = self.slots_log_sigma.expand(b, self.num_vectors, -1).exp()
        slots = mu + sigma * torch.randn_like(mu)

        # Iterative Routing (ê²½ìŸ ì‹œìž‘!)
        for _ in range(self.iters):
            slots_prev = slots
            slots = self.norm_slots(slots)
            
            # Query ìƒì„±
            q = self.to_q(slots) # (B, K, D)

            # Attention Score
            dots = torch.einsum('bid,bjd->bij', q, k) * self.scale
            
            # ðŸ”¥ ì—¬ê¸°ê°€ í•µì‹¬ ì°¨ì´ì ! ðŸ”¥
            # ì¼ë°˜ ì–´í…ì…˜: Softmax(dim=-1) -> Key(ìž…ë ¥ ë‹¨ì–´) ì¶•ìœ¼ë¡œ í™•ë¥  ê³„ì‚°
            # Slot ì–´í…ì…˜: Softmax(dim=1)  -> Slot(ì¿¼ë¦¬) ì¶•ìœ¼ë¡œ í™•ë¥  ê³„ì‚°
            # ì˜ë¯¸: "ì´ ë‹¨ì–´ëŠ” ë‚´êº¼ì•¼!" ë¼ê³  ìŠ¬ë¡¯ë¼ë¦¬ ê²½ìŸí•¨
            attn = dots.softmax(dim=1) + 1e-8 # (B, K, L)
            
            # Weighted Sum (ê·¼ë° ì´ì œ ì •ê·œí™”ë¥¼ ê³ë“¤ì¸)
            # íŠ¹ì • ìŠ¬ë¡¯ì´ ì •ë³´ë¥¼ ë„ˆë¬´ ë…ì í•˜ì§€ ì•Šê²Œ ë‚˜ëˆ ì¤Œ
            attn = attn / attn.sum(dim=-1, keepdim=True)
            
            updates = torch.einsum('bjd,bij->bid', v, attn)

            # GRUë¡œ ìŠ¬ë¡¯ ì—…ë°ì´íŠ¸ (ìž”ì°¨ ì—°ê²° ëŠë‚Œ)
            # GRUCellì€ (Batch * Num_Slots, Dim) í˜•íƒœë¥¼ ë°›ìŒ
            slots = self.gru(
                updates.reshape(-1, d),
                slots_prev.reshape(-1, d)
            ).reshape(b, self.num_vectors, d)
            
            # Optional MLP
            slots = slots + self.mlp(self.norm_slots(slots))

        return slots # (B, 2, D) -> v0, v1