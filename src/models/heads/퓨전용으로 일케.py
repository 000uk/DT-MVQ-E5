import torch
import torch.nn as nn
import torch.nn.functional as F

class CompetitiveAttention(nn.Module):
    """
    nn.MultiheadAttention ëŒ€ì‹  ì‚¬ìš©í•  'ê²½ìŸì  ì–´í…ì…˜' ëª¨ë“ˆ
    Slot Attentionì˜ í•µì‹¬ì¸ Softmax(dim=Query)ë¥¼ êµ¬í˜„í•¨
    """
    def __init__(self, input_dim, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = input_dim // num_heads
        self.scale = self.head_dim ** -0.5

        self.to_q = nn.Linear(input_dim, input_dim, bias=False)
        self.to_k = nn.Linear(input_dim, input_dim, bias=False)
        self.to_v = nn.Linear(input_dim, input_dim, bias=False)
        self.to_out = nn.Linear(input_dim, input_dim)

    def forward(self, query, key, value):
        B, K, _ = query.shape # K: ì¿¼ë¦¬ ê°œìˆ˜ (Slots)
        _, L, _ = key.shape   # L: ì‹œí€€ìŠ¤ ê¸¸ì´
        H = self.num_heads

        # 1. Projection & Head Split
        q = self.to_q(query).reshape(B, K, H, -1).permute(0, 2, 1, 3) # (B, H, K, D)
        k = self.to_k(key).reshape(B, L, H, -1).permute(0, 2, 1, 3)   # (B, H, L, D)
        v = self.to_v(value).reshape(B, L, H, -1).permute(0, 2, 1, 3) # (B, H, L, D)

        # 2. Score Calculation
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale # (B, H, K, L)

        # 3. ğŸ”¥ [í•µì‹¬] Inverted Softmax (Slot Attentionì˜ ì˜í˜¼)
        # ì¼ë°˜ Attention: softmax(dim=-1) -> ë‹¨ì–´ ì¶•ìœ¼ë¡œ í™•ë¥  (ëª¨ë“  ì¿¼ë¦¬ê°€ ê°™ì€ ë‹¨ì–´ ë´ë„ ë¨)
        # ê²½ìŸ Attention: softmax(dim=-2) -> ì¿¼ë¦¬ ì¶•ìœ¼ë¡œ í™•ë¥  (v0ê°€ ê°€ì ¸ê°€ë©´ v1ì€ ëª» ê°€ì ¸ê°!)
        attn = dots.softmax(dim=-2) 
        
        # (Optional) ì•ˆì •ì„±ì„ ìœ„í•œ Normalization (Slot Attention ë…¼ë¬¸ ë””í…Œì¼)
        # ê° ì¿¼ë¦¬ê°€ ë„ˆë¬´ ì‘ì€ ê°’ë§Œ ê°€ì ¸ê°€ì§€ ì•Šë„ë¡ ë³´ì •
        attn = attn / (attn.sum(dim=-1, keepdim=True) + 1e-8)

        # 4. Aggregation
        out = torch.matmul(attn, v) # (B, H, K, D)
        out = out.permute(0, 2, 1, 3).reshape(B, K, -1)
        return self.to_out(out), attn

class FusedMultiVectorHead(nn.Module):
    def __init__(self, num_vectors=3, input_dim=384):
        super().__init__()
        
        # 1. ì´ˆê¸°í™” (Orthogonal í•„ìˆ˜!)
        self.query_tokens = nn.Parameter(torch.randn(1, num_vectors, input_dim))
        nn.init.orthogonal_(self.query_tokens)

        # 2. ğŸ”¥ [êµì²´] ì¼ë°˜ Attention -> ê²½ìŸì  Attention
        self.attention = CompetitiveAttention(input_dim=input_dim, num_heads=8)
        
        self.norm1 = nn.LayerNorm(input_dim)
        self.dropout = nn.Dropout(0.1)

        # 3. FFN (ë‹˜ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€ - ì•„ì£¼ í›Œë¥­í•¨)
        self.norm2 = nn.LayerNorm(input_dim)
        self.ffn = nn.Sequential(
            nn.Linear(input_dim, input_dim * 4),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(input_dim * 4, input_dim),
            nn.Dropout(0.1)
        )

    def forward(self, seq_out, attn_mask=None):
        batch_size = seq_out.shape[0]
        queries = self.query_tokens.repeat(batch_size, 1, 1) # (B, K, D)

        # ê²½ìŸì  ì–´í…ì…˜ ìˆ˜í–‰
        # (ë§ˆìŠ¤í‚¹ì€ ë³µì¡í•´ì„œ ìƒëµí•´ë„ E5ê°€ ì´ë¯¸ ì˜í•´ì„œ ê´œì°®ì§€ë§Œ, í•„ìš”í•˜ë©´ attnì— -inf ì¶”ê°€)
        attn_out, _ = self.attention(query=queries, key=seq_out, value=seq_out)
        
        # Residual & Norm
        x = self.norm1(queries + self.dropout(attn_out))

        # FFN & Residual
        vectors = self.norm2(x + self.ffn(x))
        
        return vectors