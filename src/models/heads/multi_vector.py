import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleMultiVectorHead(nn.Module):
    def __init__(self, num_vectors=3,  input_dim=384):
        super().__init__()

        # kê°œì˜ í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ í† í° ìƒì„±
        self.query_tokens = nn.Parameter( # ì¼ë‹¨ í…ì„œëž‘ ë‹¤ë¥´ê²Œ í•™ìŠµ ê°€ëŠ¥í•¨
            torch.randn(1, num_vectors, input_dim) # (1, K, D) ì°¨ì›ì˜ ëžœë¤ê°’
        )
        # nn.init.normal_(self.query_tokens, std=0.02)
        # nn.init.orthogonal_(self.query_tokens)
        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=8, batch_first=True)
        self.norm = nn.LayerNorm(input_dim)
    
    def forward(self, seq_out, attn_mask):
        """
        # shapes
        query = queries  # (B, K, D)
        key = seq_out    # (B, L, D)
        value = seq_out  # (B, L, D)
        """
        batch_size = seq_out.shape[0] # (B)
        queries = self.query_tokens.repeat(batch_size, 1, 1)  # Query í™•ìž¥ (1, K, D) -> (B, K, D)
        key_padding_mask = ~attn_mask.bool()

        vectors, _ = self.attention( # (B, K, D)
            query=queries,
            key=seq_out,
            value=seq_out,
            key_padding_mask=key_padding_mask
        )

        # 2. ðŸ”¥ [Pro Tip] ìž”ì°¨ ì—°ê²° (Residual) + LayerNorm
        # "ìƒˆë¡œ ë°°ìš´ ì •ë³´(vectors)ì— ì›ëž˜ ë‚´ ìžì•„(queries)ë¥¼ ì„žëŠ”ë‹¤"
        # ì´ë ‡ê²Œ í•˜ë©´ í•™ìŠµì´ í›¨ì”¬ ì•ˆì •ì ìœ¼ë¡œ ë³€í•©ë‹ˆë‹¤.
        # vectors = self.norm(queries + vectors)
        return vectors

class MultiVectorHead(nn.Module): # transformerì—ì„œ self_attnë§Œ ì œê±°í•¨
    def __init__(self, num_vectors=3,  input_dim=384):
        super().__init__()

        # kê°œì˜ í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ í† í° ìƒì„±
        self.query_tokens = nn.Parameter( # ì¼ë‹¨ í…ì„œëž‘ ë‹¤ë¥´ê²Œ í•™ìŠµ ê°€ëŠ¥í•¨
            torch.randn(1, num_vectors, input_dim) # (1, K, D) ì°¨ì›ì˜ ëžœë¤ê°’
        )
        # nn.init.normal_(self.query_tokens, std=0.02)

        # Cross-Attention (ì •ë³´ ìˆ˜ì§‘)
        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=8, batch_first=True)
        self.norm1 = nn.LayerNorm(input_dim)
        self.dropout = nn.Dropout(0.1)
        
        # FFN (ìˆ˜ì§‘í•œ ì •ë³´ ê°€ê³µ)
        self.norm2 = nn.LayerNorm(input_dim)
        self.ffn = nn.Sequential(
            nn.Linear(input_dim, input_dim * 4),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(input_dim * 4, input_dim),
            nn.Dropout(0.1)
        )

    def forward(self, seq_out, attn_mask):
        """
        # shapes
        query = queries  # (B, K, D)
        key = seq_out    # (B, L, D)
        value = seq_out  # (B, L, D)
        """
        batch_size = seq_out.shape[0] # (B)
        queries = self.query_tokens.repeat(batch_size, 1, 1)  # Query í™•ìž¥ (1, K, D) -> (B, K, D)
        key_padding_mask = ~attn_mask.bool()

        attn_out, _ = self.attention(
            query=queries,
            key=seq_out,
            value=seq_out,
            key_padding_mask=key_padding_mask
        )
        x = self.norm1(queries + self.dropout(attn_out))

        # vectors = x + self.ffn(self.norm2(x))
        ffn_out = self.ffn(x)
        vectors = self.norm2(x + ffn_out)
        return vectors